import { createHash } from 'crypto';
import { createReadStream, createWriteStream, statSync, existsSync, writeFileSync, readFileSync, unlinkSync } from 'fs';
import { readdir } from 'fs/promises';
import { tmpdir } from 'os';
import { join, resolve, relative } from 'path';
import archiver from 'archiver';
import axios from 'axios';
import BeamClient from './index';

// Global workspace object id to signal to any other threads that the workspace has already been synced
let _workspaceObjectId = "";
let _syncLock = false;

export function setWorkspaceObjectId(objectId: string): void {
  _workspaceObjectId = objectId;
}

export function getWorkspaceObjectId(): string {
  return _workspaceObjectId || "";
}

function ignoreFileName(): string {
  return '.beamignore';
}

function ignoreFileContents(): string {
  return `# Generated by Beam SDK
.beamignore
pyproject.toml
.git
.idea
.python-version
.vscode
.venv
venv
__pycache__
.DS_Store
.config
drive/MyDrive
.coverage
.pytest_cache
.ipynb
.ruff_cache
.dockerignore
.ipynb_checkpoints
.env.local
.envrc
**/__pycache__/
**/.pytest_cache/
**/node_modules/
**/.venv/
*.pyc
.next/
.circleci
`;
}

export interface ObjectMetadata {
  name: string;
  size: number;
}

export interface CreateObjectRequest {
  object_metadata: ObjectMetadata;
  hash: string;
  size: number;
  overwrite: boolean;
}

export interface CreateObjectResponse {
  ok: boolean;
  objectId: string;
  presignedUrl: string;
}

export interface HeadObjectRequest {
  hash: string;
}

export interface HeadObjectResponse {
  exists: boolean;
  ok: boolean;
  objectId: string;
  objectMetadata: any | null;
  errorMsg: string;
  useWorkspaceStorage: boolean;
}

export interface PutObjectRequest {
  chunk: Buffer;
  metadata: ObjectMetadata;
  hash: string;
  is_final: boolean;
}

export interface FileSyncResult {
  success: boolean;
  object_id: string;
}

export class FileSyncer {
  private rootDir: string;
  private client: BeamClient;
  private isWorkspaceDir: boolean;
  private ignorePatterns: string[] = [];
  private includePatterns: string[] = [];

  constructor(client: BeamClient, rootDir: string = ".") {
    this.rootDir = resolve(rootDir);
    this.client = client;
    this.isWorkspaceDir = rootDir === ".";
  }

  private get ignoreFilePath(): string {
    return join(this.rootDir, ignoreFileName());
  }

  private initIgnoreFile(): void {
    if (existsSync(this.ignoreFilePath)) {
      return;
    }

    console.log(`Writing ${ignoreFileName()} file`);
    writeFileSync(this.ignoreFilePath, ignoreFileContents());
  }

  private readIgnoreFile(): string[] {
    console.log(`Reading ${ignoreFileName()} file`);
    
    const patterns: string[] = [];
    
    if (existsSync(this.ignoreFilePath)) {
      const content = readFileSync(this.ignoreFilePath, 'utf-8');
      return content
        .split('\n')
        .map(line => line.trim())
        .filter(line => line && !line.startsWith('#'));
    }
    
    return patterns;
  }

  private shouldIgnore(filePath: string): boolean {
    const relativePath = relative(this.rootDir, filePath);
    
    // Simple pattern matching - could be enhanced with proper glob matching
    return this.ignorePatterns.some(pattern => {
      if (pattern === '*') return true;
      if (pattern.startsWith('**/')) {
        return relativePath.includes(pattern.slice(3));
      }
      if (pattern.endsWith('/**')) {
        return relativePath.startsWith(pattern.slice(0, -3));
      }
      if (pattern.includes('*')) {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(relativePath);
      }
      return relativePath === pattern || relativePath.startsWith(pattern + '/');
    });
  }

  private shouldInclude(filePath: string): boolean {
    if (this.includePatterns.length === 0) {
      return true;
    }

    const relativePath = relative(this.rootDir, filePath);
    
    return this.includePatterns.some(pattern => {
      if (pattern.includes('*')) {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(relativePath);
      }
      return relativePath === pattern || relativePath.startsWith(pattern + '/');
    });
  }

  private async *collectFiles(): AsyncGenerator<string> {
    if (this.ignorePatterns.includes('*')) {
      return;
    }

    console.log(`Collecting files from ${this.rootDir}`);

    const walk = async (dir: string): Promise<string[]> => {
      const files: string[] = [];
      try {
        const entries = await readdir(dir, { withFileTypes: true });
        
        for (const entry of entries) {
          const fullPath = join(dir, entry.name);
          
          if (entry.isDirectory()) {
            if (!this.shouldIgnore(fullPath)) {
              const subFiles = await walk(fullPath);
              files.push(...subFiles);
            }
          } else if (entry.isFile()) {
            if (!this.shouldIgnore(fullPath) && this.shouldInclude(fullPath)) {
              files.push(fullPath);
            }
          }
        }
      } catch (error) {
        console.warn(`Failed to read directory ${dir}: ${error}`);
      }
      return files;
    };

    const allFiles = await walk(this.rootDir);
    for (const file of allFiles) {
      yield file;
    }
  }

  private static calculateSha256(filePath: string): Promise<string> {
    return new Promise((resolve, reject) => {
      const hash = createHash('sha256');
      const stream = createReadStream(filePath);
      
      stream.on('error', reject);
      stream.on('data', chunk => hash.update(chunk));
      stream.on('end', () => resolve(hash.digest('hex')));
    });
  }

  private async createZipFile(): Promise<{ filePath: string; size: number; hash: string }> {
    const tempZipPath = join(tmpdir(), `beam-sync-${Date.now()}.zip`);
    const output = createWriteStream(tempZipPath);
    const archive = archiver('zip', {
      zlib: { level: 9 }, // Maximum compression
      forceLocalTime: true, // Better compatibility
      store: false // Always compress
    });

    let totalFiles = 0;
    let processedFiles = 0;

    // Enhanced event handling
    archive.on('error', (err) => {
      console.error('Archive error:', err);
      throw err;
    });

    archive.on('warning', (err) => {
      if (err.code === 'ENOENT') {
        console.warn('File not found (skipping):', err.path);
      } else {
        console.warn('Archive warning:', err);
      }
    });

    // Progress tracking
    archive.on('entry', (entry) => {
      processedFiles++;
      if (totalFiles > 0) {
        const progress = ((processedFiles / totalFiles) * 100).toFixed(1);
        console.log(`Progress: ${progress}% (${processedFiles}/${totalFiles}) - ${entry.name}`);
      }
    });

    // Handle output stream errors
    output.on('error', (err) => {
      console.error('Output stream error:', err);
      throw err;
    });

    // Pipe archive data to the file
    archive.pipe(output);

    // Count total files first for progress tracking
    const allFiles: string[] = [];
    for await (const filePath of this.collectFiles()) {
      allFiles.push(filePath);
    }
    totalFiles = allFiles.length;

    console.log(`Archiving ${totalFiles} files...`);

    // Add files to archive using streaming (memory efficient)
    for (const filePath of allFiles) {
      try {
        const relativePath = relative(this.rootDir, filePath);
        const stats = statSync(filePath);
        
        if (stats.isFile()) {
          // Stream files instead of loading into memory
          // This preserves file timestamps and permissions
          archive.file(filePath, { 
            name: relativePath,
            date: stats.mtime // Preserve modification time
          });
        }
      } catch (error) {
        console.warn(`Failed to add ${filePath}: ${error}`);
      }
    }

    // Finalize the archive - this triggers the actual compression
    console.log('Finalizing archive...');
    await archive.finalize();

    // Wait for the output stream to close
    await new Promise((resolve, reject) => {
      output.on('close', () => {
        console.log('Archive created successfully');
        resolve(undefined);
      });
      output.on('error', reject);
    });

    const stats = statSync(tempZipPath);
    const hash = await FileSyncer.calculateSha256(tempZipPath);

    // Log compression statistics
    const compressionRatio = archive.pointer() > 0 ? ((stats.size / archive.pointer()) * 100).toFixed(1) : '0';
    console.log(`Archive stats: ${this.formatBytes(stats.size)} (${compressionRatio}% compression ratio)`);

    return {
      filePath: tempZipPath,
      size: stats.size,
      hash
    };
  }

  private async headObject(hash: string): Promise<HeadObjectResponse> {
    console.log("workspaceId", this.client.opts.workspaceId);
    const response = await this.client.request({
      method: 'GET',
      url: `/api/v1/gateway/objects/${hash}`
    });

    return response.data;
  }

  private async createObject(request: CreateObjectRequest): Promise<CreateObjectResponse> {
    const response = await this.client.request({
      method: 'POST',
      url: `/api/v1/gateway/objects`,
      data: request
    });

    return response.data;
  }

  private async uploadToPresignedUrl(presignedUrl: string, filePath: string): Promise<boolean> {
    try {
      const stats = statSync(filePath);
      const fileStream = createReadStream(filePath);
      
      console.log(`Uploading ${this.formatBytes(stats.size)} to cloud storage...`);
      
      await axios.put(presignedUrl, fileStream, {
        headers: {
          'Content-Type': 'application/zip',
          'Content-Length': stats.size.toString()
        },
        maxBodyLength: Infinity,
        maxContentLength: Infinity,
        timeout: 300000, // 5 minute timeout for large files
        onUploadProgress: (progressEvent) => {
          if (progressEvent.total) {
            const progress = ((progressEvent.loaded / progressEvent.total) * 100).toFixed(1);
            console.log(`Upload progress: ${progress}% (${this.formatBytes(progressEvent.loaded)}/${this.formatBytes(progressEvent.total)})`);
          }
        }
      });

      console.log('Upload completed successfully ✅');
      return true;
    } catch (error) {
      console.error('Upload failed:', error);
      return false;
    }
  }

  private formatBytes(bytes: number): string {
    const sizes = ['Bytes', 'KB', 'MB', 'GB'];
    if (bytes === 0) return '0 Bytes';
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];
  }

  public async sync(
    ignorePatterns: string[] = [],
    includePatterns: string[] = [],
    cacheObjectId: boolean = true
  ): Promise<FileSyncResult> {
    // Simple lock mechanism
    if (_syncLock) {
      console.log('Sync already in progress');
      return { success: false, object_id: '' };
    }

    if (this.isWorkspaceDir && getWorkspaceObjectId() !== '') {
      console.log('Files already synced');
      return { success: true, object_id: getWorkspaceObjectId() };
    }

    _syncLock = true;
    
    try {
      return await this._sync(ignorePatterns, includePatterns, cacheObjectId);
    } finally {
      _syncLock = false;
    }
  }

  private async _sync(
    ignorePatterns: string[] = [],
    includePatterns: string[] = [],
    cacheObjectId: boolean = true
  ): Promise<FileSyncResult> {
    console.log('Syncing files');

    this.initIgnoreFile();

    if (!ignorePatterns || ignorePatterns.length === 0) {
      this.ignorePatterns = this.readIgnoreFile();
    } else {
      this.ignorePatterns = ignorePatterns;
    }

    if (!includePatterns || includePatterns.length === 0) {
      this.includePatterns = [];
    } else {
      this.includePatterns = includePatterns;
    }

    const { filePath: tempZipPath, size, hash } = await this.createZipFile();

    if (this.ignorePatterns[0] !== '*') {
      console.log(`Collected object is ${this.formatBytes(size)}`);
    }

    let objectId: string | null = null;

    try {
      const headResponse = await this.headObject(hash);
      console.log("headResponse", headResponse);
      
      if (!headResponse.exists) {
        const metadata: ObjectMetadata = { name: hash, size };

        console.log('Uploading');
        
        if (headResponse.useWorkspaceStorage) {
          const createResponse = await this.createObject({
            object_metadata: metadata,
            hash,
            size,
            overwrite: true
          });

          if (createResponse.ok) {
            console.log("createResponse", createResponse);
            const uploadSuccess = await this.uploadToPresignedUrl(createResponse.presignedUrl, tempZipPath);
            
            if (uploadSuccess) {
              if (this.isWorkspaceDir && cacheObjectId) {
                setWorkspaceObjectId(createResponse.objectId);
              }
              objectId = createResponse.objectId;
            } else {
              console.error('File sync failed here');
            }
          }
        } else {
          // TODO: Implement streaming upload for older workspaces

          console.error('Streaming upload not yet implemented');
        }
      } else if (headResponse.exists && headResponse.ok) {
        console.log('Files already synced');

        if (this.isWorkspaceDir && cacheObjectId) {
          setWorkspaceObjectId(headResponse.objectId);
        }
        objectId = headResponse.objectId;
      }
    } finally {
      // Clean up temporary file
      try {
        unlinkSync(tempZipPath);
      } catch (error) {
        console.warn(`Failed to clean up temporary file: ${error}`);
      }
    }

    if (objectId === null) {
      console.error('File sync failed');
      return { success: false, object_id: '' };
    }

    console.log('Files synced');
    return { success: true, object_id: objectId };
  }
}
